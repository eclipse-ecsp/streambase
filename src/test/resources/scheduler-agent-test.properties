#################################################################################################################
#Stream processor properties
#Below are the required properties for the stream processors to run
#################################################################################################################
launcher.impl.class.fqn=org.eclipse.ecsp.analytics.stream.base.KafkaStreamsLauncher
post.processors=org.eclipse.ecsp.analytics.stream.base.processors.SchedulerAgentPostProcessor,org.eclipse.ecsp.stream.scheduler.SchedulerAgentIntegrationTest$TestStreamPostProcessor
#Below property specify the Fully qualified name of your processor(s) classes. Currently single processor is supported
service.stream.processors=org.eclipse.ecsp.stream.scheduler.SchedulerAgentIntegrationTest$SchedulerAgentTestStreamProcessor
#The input source topic your stream processor listens to
source.topic.name=schedulerAgentSource
#The sink topic where the stream processor will push the dta to
sink.topic.name=schedulerAgentSink
#Scheduler Agent Stream Processor
scheduler.agent.topic.name=scheduler
#application id / consumer group of your stream processor
application.id=scheduler-agent-test-sp
#Service name
service.name=SchedulerAgentTestService
#Comma separated list of kafka brokers
bootstrap.servers=localhost:9092
#Comma separated list of zookeepers
zookeeper.connect=localhost:2181
#Number of parallelism
num.stream.threads=1
#State store directory
state.dir=/tmp/kafka-streams
#Number of records that will be polled from kafka topic
max.poll.records=1000
session.timeout.ms=30000
request.timeout.ms=40000
kafka.rebalance.time.mins=10
kafka.close.timeout.secs=30
#setting this to nowhere means reset will not happen (earliest,latest,nowhere)
application.offset.reset=nowhere
#Set the below property as true, if changeLog topic is to be created for the state store
state.store.changelog.enabled=false
#################################################################################################################
#Transformer properties
#Developer should provide the logic to convert byte[] to Igniteevent
#################################################################################################################
#Provide the custom implementation of how the byte[] (JSON) should be converted to Ignite event
event.transformer.classes=genericIgniteEventTransformer
#Provide the custom implementation of how the byte[](JSON) should be converted to Ignite key
ignite.key.transformer.class=org.eclipse.ecsp.transform.IgniteKeyTransformerStringImpl
device.messaging.event.transformer.class=org.eclipse.ecsp.transform.DeviceMessageIgniteEventTransformer
#################################################################################################################
#Metric Properties
#In case you want to capture metrics for your stream processor
#################################################################################################################
metric.reporters=org.eclipse.ecsp.analytics.stream.base.metrics.reporter.ConsoleMetricReporter
metrics.sample.window.ms=60000
metrics.num.samples=15000
#Kafka-redis-connector metrics
#define the interval of logging
metric.logging.interval=2
#define the time units. Possible values supported are minutes,seconds, hours, milliseconds
metric.logging.unit=minutes
#metrics for reporting the number of events getting processed per second
metrics.event.rate.enable=true
#metrics for reporting the number of events processed by redis per second
metrics.event.rate.redis.enable=true
#metrics for reporting the average latency in events processing for redis
metrics.avg.latency.redis.enable=true
#################################################################################################################
#Mqtt Properties
#You should configure the mqtt properties in case you wanted to send some data to Device (via mqtt)
# Used by DeviceMessaging agent
#################################################################################################################
mqtt.short.circuit=true
mqtt.broker.url=tcp://127.0.0.1:1883
# separator is defaulted to /
mqtt.topic.separator=/
mqtt.config.qos=1
mqtt.user.name=8146ccc47e84ac1e43de623403133d55
mqtt.user.password=simulator16
mqtt.service.topic.name.prefix=prefix/
mqtt.service.topic.name=testSource
#################################################################################################################
#Cumulative logging Properties
#You should configure the below properties in case you want cumlative logging (CLOGGER)
#################################################################################################################
#Cumulative logging configuration
log.counts=true
log.counts.minutes=1
log.per.pdid=false
discovery.impl.class.fqn=org.eclipse.ecsp.analytics.stream.base.discovery.PropBasedDiscoveryServiceImpl
#Serialization class
ingestion.serializer.class=org.eclipse.ecsp.serializer.IngestionSerializerFstImpl
#SSL Configuration
kafka.ssl.enable=false
kafka.ssl.client.auth=required
kafka.client.keystore=/kafka/ssl/kafka.client.keystore.jks
kafka.client.keystore.password=shcuwNHARcNuag8SgYdsG8cWuPExY3Tx
kafka.client.key.password=pUBPHXM9mP5PrRBrTEpF5cV2TpjvWtb5
kafka.client.truststore=/kafka/ssl/kafka.client.truststore.jks
kafka.client.truststore.password=9vq9ghbSFd7JMFSgGMSCEuAzE3q27Xd3
#Mongo Configuration
mongodb.hosts=localhost
mongodb.port=27017
mongodb.username=admin
mongodb.password=password
mongodb.auth.db=admin
mongodb.name=test
mongodb.pool.max.size=200
mongodb.max.wait.time.ms=60000
mongodb.connection.timeout.ms=60000
mongodb.socket.timeout.ms=60000
mongodb.max.connections.per.host=200
mongodb.block.threads.allowed.multiplier=10
mongodb.read.preference=secondaryPreferred
morphia.map.packages=org.eclipse.ecsp.dao
mongodb.server.selection.timeout=30000
dma.service.max.retry=0
mongodb.taggable.read.preference.enabled=false
mongodb.read.preference.tag=primary_region
#The modes are SINGLE,REPLICA,CLUSTER,SENTINEL
redis.address=127.0.0.1:6379
redis.sentinels=
redis.master.name=
redis.dns.monitoring.interval=5000
redis.read.mode=SLAVE
redis.subscription.mode=SLAVE
redis.subscription.conn.min.idle.size=1
redis.subscription.conn.pool.size=50
redis.slave.conn.min.idle.size=32
redis.slave.pool.size=64
redis.master.conn.min.idle.size=32
redis.master.conn.pool.size=64
redis.idle.conn.timeout=10000
redis.conn.timeout=10000
redis.timeout=3000
redis.retry.attempts=3
redis.retry.interval=1500
redis.reconnection.timeout=3000
redis.failed.attempts=3
redis.database=0
redis.password=
redis.subscriptions.per.conn=5
redis.client.name=yellow
redis.conn.min.idle.size=32
redis.conn.pool.size=64
redis.cluster.masters=
redis.scan.interval=1000
redis.scan.limit=10
redis.regex.scan.filename=scanregex.txt
redis.pipeline.size=2
redis.netty.threads=0
redis.decode.in.executor=true
// Added because DMA pre & post processors are not part of chain
start.device.status.consumer=false
prometheus.agent.port=9100
metrics.prometheus.enabled=false
prometheus.histogram.buckets=0.005, 0.010, 0.015, 0.020, 0.025, 0.030, 0.080, 0.1, 0.2, 0.3
health.mqtt.monitor.enabled=false
health.mongo.monitor.enabled=false
health.kafka.consumer.group.monitor.enabled=false
health.device.status.backdoor.monitor.enabled=false
health.kafka.topics.monitor.enabled=false
health.redis.monitor.enabled=false
#Enabling DMA/SCHEDULER Module Configurations For StreamBase
dma.enabled=false
scheduler.enabled=false
# Configure whether event needs to be deleted from MongoDB after TTL expiration
dma.remove.on.ttl.expiry.enabled=true